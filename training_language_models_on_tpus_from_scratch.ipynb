{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "training-language-models-on-tpus-from-scratch.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shadow977/BERT-training-language-models-on-tpus-from-scratch.ipynb/blob/master/training_language_models_on_tpus_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoQebUU1vWqu",
        "colab_type": "text"
      },
      "source": [
        "#### In this kernel, I will show you how to train language models, such as BERT, from scratch on TPUs!\n",
        "\n",
        "#### you can find the bert-base data sets here:(Rest are in repository)\n",
        "\n",
        "- https://www.kaggle.com/abhishek/bert-base-uncased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFfMXSBi6Sys",
        "colab_type": "text"
      },
      "source": [
        "Update your tokenizers to add support for hugging face tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "y7ATRjf_vWqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -U tokenizers==0.4.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqCjIixi6SP1",
        "colab_type": "text"
      },
      "source": [
        "Since huggingface library doesnt support TF2.0,Downgrade tensorflow to 1.15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "4UgHxQsAvWq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szSR_2HNvWrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBZ_4Lyz6snT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUj9icfovWrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bwpt = tokenizers.BertWordPieceTokenizer(\n",
        "    vocab_file=None,\n",
        "    add_special_tokens=True,\n",
        "    unk_token='[UNK]',\n",
        "    sep_token='[SEP]',\n",
        "    cls_token='[CLS]',\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=True,\n",
        "    strip_accents=True,\n",
        "    lowercase=True,\n",
        "    wordpieces_prefix='##'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ-fHmqK9qgK",
        "colab_type": "text"
      },
      "source": [
        "Import BERT source and BERT base uncased files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R10ITn3B2EDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import shutil\n",
        "shutil.copy('/content/gdrive/My Drive/Collab Data/BERT_base_uncased.zip','/content/')\n",
        "shutil.copy('/content/gdrive/My Drive/Collab Data/BERTsrc.zip','/content/')\n",
        "\n",
        "!unzip BERT_base_uncased.zip\n",
        "!unzip '/content/BERTsrc.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OyVaKDS9wmg",
        "colab_type": "text"
      },
      "source": [
        "Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VvP-WarvarV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/Shadow977/BERT-training-language-models-on-tpus-from-scratch.ipynb/blob/master/hi_dedup_1000.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jLd_iuU90Mk",
        "colab_type": "text"
      },
      "source": [
        "Train tokenizer on dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoGrlNI3vWrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bwpt.train(\n",
        "    files=[\"/content/hi_dedup_1000.txt\"],\n",
        "    vocab_size=30000,\n",
        "    min_frequency=3,\n",
        "    limit_alphabet=1000,\n",
        "    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[MASK]', '[SEP]']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFpV8oWl93Ur",
        "colab_type": "text"
      },
      "source": [
        "Save the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiuDvu8lvWrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bwpt.save(\"/content/\", \"hindi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RHb-yQ9vWrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7mu3g6WvWra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGl0ivvmvWre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python create_pretraining_data.py \\\n",
        "    --input_file=hi_dedup_1000.txt \\\n",
        "    --output_file=tf_examples.tfrecord \\\n",
        "    --vocab_file=hindi-vocab.txt \\\n",
        "    --do_lower_case=True \\\n",
        "    --max_seq_length=128 \\\n",
        "    --max_predictions_per_seq=20 \\\n",
        "    --masked_lm_prob=0.15 \\\n",
        "    --random_seed=42 \\\n",
        "    --dupe_factor=5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEgmGwe07qj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4fpFpBT8JWE",
        "colab_type": "text"
      },
      "source": [
        "To access the storage you need to authenticate and get storage bucket, for that you'll need colab free trail or colab pro.\n",
        "you can get the bucket name [here](https://console.cloud.google.com/storage/).\n",
        "\n",
        "`where the SERVICE_ACCOUNT is service-[PROJECT_NUMBER]@cloud-tpu.iam.gserviceaccount.com`\n",
        "\n",
        "The PROJECT_NUMBER for Colab is contained in the error message of the response when TPU is not authenticated.\n",
        "\n",
        "So, first run your TPU service without TPU auth, waiting for the error message from the response, then Enable TPU auth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmmti9Id72B1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil ls gs://[BUCKET_NAME]\n",
        "gsutil acl ch -u [SERVICE_ACCOUNT]:READER gs://[BUCKET_NAME]\n",
        "gsutil acl ch -u [SERVICE_ACCOUNT]:WRITER gs://[BUCKET_NAME]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3pzfwEcvWri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run_pretraining.py \\\n",
        "    --input_file=gs://tf-lang-model/*.tfrecord \\\n",
        "    --output_dir=gs://tf-lang-model/model/ \\\n",
        "    --do_train=True \\\n",
        "    --do_eval=True \\\n",
        "    --bert_config_file=/content/config.json \\\n",
        "    --train_batch_size=32 \\\n",
        "    --max_seq_length=128 \\\n",
        "    --max_predictions_per_seq=20 \\\n",
        "    --num_train_steps=20 \\\n",
        "    --num_warmup_steps=10 \\\n",
        "    --learning_rate=2e-5 \\\n",
        "    --use_tpu=True \\\n",
        "    --tpu_name=$TPU_NAME"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}